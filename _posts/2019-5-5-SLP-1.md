---
layout:     post
title:      "Speech and Language Processing (chapter1)"
subtitle:   ""
author:     "xyc"
header-img: "img/svg/banner.jpg"
header-mask:  0.5
catalog: true
tags:
    - Speech and Language Processing
---

# 前言
Speech and Language Processing(3rd ed. draft)阅读笔记
## Chapter 1 Regular Expressions, Text Normalization, Edit Distance
1.第一章的重点介绍了语言处理的一个重要工具，正则表达式(regular expression)。  
2.一个统称的任务-Text Normalization,包括:word segmentation and normalization, sentence segmentation, and stemming  
### Regular Expressions
搜索指定字符串的语言
### Words
- **corpus(语料库)**: 计算机可读的文本或语音集合。  
语料库中的一段例子：I do uh main- mainly business data processing.   
这句话有两处**不流畅的地方(disfluencies)**, main- 称为**fragment**(碎片？), uh称为 **filters or filled paused(填充词)** 。  
- **lemma(词元?)** : a set of lexical forms having the same stem, the same major part-of-speech, and the same word sense.  
例子： sings，sung,sang ->sing  
对形态复杂的语言，如阿拉伯语，我们需要做**lemmatization（词形还原）** ，有些语言，如英语，使用 **wordform(词形)** 就足够了。  
- **word type**: 语料库中独立词语的数目(不计算重复出现)。  
- **word token**: 语料库中词语总数(计算重复)。  
例子:They picnicked by the pool, then lay back on the grass and looked at the stars(16 tokens and 14 types)
 - **Herdan’s Law**： relationship between the number of types $|V|$ and the number of tokens $N$  
 $$|V| = kN^{\beta}$$
 
 ### Text Normalization(文本规范化)
 - **tokenizaiton(分词)** ： 将文本分成词语
 - **normalizaiton(规范化)** : 将词语转化为规范的形式
 - **clitic(附着语素)** : 只能依附其它单词出现的部分。例如: we're
 - **Case folding(大写转换)** :对语音识别、信息检索而言只是小写的映射，对情感分析、文本分类、信息提取、机器翻译而言，case folding会丢失信息.
 - **中文分割(the MaxMatch 算法)** : 需要一个词汇表，贪心算法搜索文本中和词汇表匹配的最长token进行分割.
 - **如何进行leammatization?** :  **morphological parsing(形态分析)**  
**morpheme(语素)** : 由两部分组成. 例如: cats, **stems** :cat, **affixes** : s.
**The Porter Stemmer** :简单粗暴的简化单词后缀，准确率不够高
- **Byte-Pair Encoding** ：迭代的融合出现最频繁的字符对.优点：可以处理unknow word
### Minimum Edit Distance ###
定义: 将一个字符串转化为另一个的最小操作数(插入、删除、替换)
